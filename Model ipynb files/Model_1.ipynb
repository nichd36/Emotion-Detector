{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the mode to either train or display and changing the batch size and number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"train\"\n",
    "batch_size = 32\n",
    "num_epoch = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28821 images belonging to 7 classes.\n",
      "Found 7066 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'images/train'\n",
    "val_dir = 'images/validation'\n",
    "\n",
    "num_train = 28709\n",
    "num_val = 7178\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=batch_size,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=batch_size,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When mode = train, it will train the model. If model = display, it will open up the camera and recognize the emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f6/mgwjx_cs0bl6g_dbgdp8jfq40000gn/T/ipykernel_91617/1110841621.py:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model_info = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "897/897 [==============================] - ETA: 0s - loss: 1.7756 - accuracy: 0.2763WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 224 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 224 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "897/897 [==============================] - 130s 145ms/step - loss: 1.7756 - accuracy: 0.2763 - val_loss: 1.6446 - val_accuracy: 0.3731\n",
      "Epoch 2/50\n",
      "897/897 [==============================] - 156s 174ms/step - loss: 1.5770 - accuracy: 0.3908\n",
      "Epoch 3/50\n",
      "897/897 [==============================] - 148s 165ms/step - loss: 1.4727 - accuracy: 0.4345\n",
      "Epoch 4/50\n",
      "897/897 [==============================] - 206s 230ms/step - loss: 1.3967 - accuracy: 0.4682\n",
      "Epoch 5/50\n",
      "897/897 [==============================] - 216s 240ms/step - loss: 1.3317 - accuracy: 0.4944\n",
      "Epoch 6/50\n",
      "897/897 [==============================] - 240s 267ms/step - loss: 1.2808 - accuracy: 0.5133\n",
      "Epoch 7/50\n",
      "897/897 [==============================] - 243s 271ms/step - loss: 1.2350 - accuracy: 0.5319\n",
      "Epoch 8/50\n",
      "897/897 [==============================] - 258s 287ms/step - loss: 1.1921 - accuracy: 0.5503\n",
      "Epoch 9/50\n",
      "897/897 [==============================] - 250s 279ms/step - loss: 1.1569 - accuracy: 0.5637\n",
      "Epoch 10/50\n",
      "897/897 [==============================] - 503s 561ms/step - loss: 1.1191 - accuracy: 0.5806\n",
      "Epoch 11/50\n",
      "897/897 [==============================] - 150s 167ms/step - loss: 1.0897 - accuracy: 0.5926\n",
      "Epoch 12/50\n",
      "897/897 [==============================] - 332s 172ms/step - loss: 1.0626 - accuracy: 0.6011\n",
      "Epoch 13/50\n",
      "897/897 [==============================] - 172s 191ms/step - loss: 1.0288 - accuracy: 0.6149\n",
      "Epoch 14/50\n",
      "897/897 [==============================] - 202s 225ms/step - loss: 1.0070 - accuracy: 0.6253\n",
      "Epoch 15/50\n",
      "897/897 [==============================] - 216s 241ms/step - loss: 0.9732 - accuracy: 0.6375\n",
      "Epoch 16/50\n",
      "897/897 [==============================] - 237s 264ms/step - loss: 0.9477 - accuracy: 0.6467\n",
      "Epoch 17/50\n",
      "897/897 [==============================] - 233s 259ms/step - loss: 0.9169 - accuracy: 0.6574\n",
      "Epoch 18/50\n",
      "897/897 [==============================] - 747s 833ms/step - loss: 0.8920 - accuracy: 0.6688\n",
      "Epoch 19/50\n",
      "897/897 [==============================] - 148s 165ms/step - loss: 0.8637 - accuracy: 0.6808\n",
      "Epoch 20/50\n",
      "897/897 [==============================] - 1000s 1s/step - loss: 0.8356 - accuracy: 0.6932\n",
      "Epoch 21/50\n",
      "897/897 [==============================] - 984s 1s/step - loss: 0.8086 - accuracy: 0.7055\n",
      "Epoch 22/50\n",
      "897/897 [==============================] - 703s 784ms/step - loss: 0.7804 - accuracy: 0.7143\n",
      "Epoch 23/50\n",
      "897/897 [==============================] - 148s 165ms/step - loss: 0.7525 - accuracy: 0.7256\n",
      "Epoch 24/50\n",
      "897/897 [==============================] - 989s 1s/step - loss: 0.7271 - accuracy: 0.7341\n",
      "Epoch 25/50\n",
      "897/897 [==============================] - 987s 1s/step - loss: 0.7009 - accuracy: 0.7429\n",
      "Epoch 26/50\n",
      "897/897 [==============================] - 982s 1s/step - loss: 0.6767 - accuracy: 0.7544\n",
      "Epoch 27/50\n",
      "897/897 [==============================] - 987s 1s/step - loss: 0.6419 - accuracy: 0.7655\n",
      "Epoch 28/50\n",
      "897/897 [==============================] - 701s 782ms/step - loss: 0.6218 - accuracy: 0.7750\n",
      "Epoch 29/50\n",
      "897/897 [==============================] - 428s 477ms/step - loss: 0.5956 - accuracy: 0.7822\n",
      "Epoch 30/50\n",
      "897/897 [==============================] - 700s 781ms/step - loss: 0.5691 - accuracy: 0.7951\n",
      "Epoch 31/50\n",
      "897/897 [==============================] - 984s 1s/step - loss: 0.5455 - accuracy: 0.8014\n",
      "Epoch 32/50\n",
      "897/897 [==============================] - 986s 1s/step - loss: 0.5275 - accuracy: 0.8088\n",
      "Epoch 33/50\n",
      "897/897 [==============================] - 980s 1s/step - loss: 0.5081 - accuracy: 0.8147\n",
      "Epoch 34/50\n",
      "897/897 [==============================] - 2246s 3s/step - loss: 0.4842 - accuracy: 0.8254\n",
      "Epoch 35/50\n",
      "897/897 [==============================] - 2042s 2s/step - loss: 0.4665 - accuracy: 0.8327\n",
      "Epoch 36/50\n",
      "897/897 [==============================] - 2478s 3s/step - loss: 0.4487 - accuracy: 0.8366\n",
      "Epoch 37/50\n",
      "897/897 [==============================] - 3019s 3s/step - loss: 0.4339 - accuracy: 0.8437\n",
      "Epoch 38/50\n",
      "897/897 [==============================] - 3042s 3s/step - loss: 0.4200 - accuracy: 0.8489\n",
      "Epoch 39/50\n",
      "897/897 [==============================] - 2078s 2s/step - loss: 0.4090 - accuracy: 0.8514\n",
      "Epoch 40/50\n",
      "897/897 [==============================] - 2933s 3s/step - loss: 0.3775 - accuracy: 0.8638\n",
      "Epoch 41/50\n",
      "897/897 [==============================] - 2982s 3s/step - loss: 0.3723 - accuracy: 0.8683\n",
      "Epoch 42/50\n",
      "897/897 [==============================] - 207s 231ms/step - loss: 0.3531 - accuracy: 0.8731\n",
      "Epoch 43/50\n",
      "897/897 [==============================] - 154s 171ms/step - loss: 0.3491 - accuracy: 0.8735\n",
      "Epoch 44/50\n",
      "897/897 [==============================] - 165s 184ms/step - loss: 0.3312 - accuracy: 0.8816\n",
      "Epoch 45/50\n",
      "897/897 [==============================] - 184s 205ms/step - loss: 0.3230 - accuracy: 0.8816\n",
      "Epoch 46/50\n",
      "897/897 [==============================] - 176s 196ms/step - loss: 0.3112 - accuracy: 0.8873\n",
      "Epoch 47/50\n",
      "897/897 [==============================] - 176s 196ms/step - loss: 0.3038 - accuracy: 0.8891\n",
      "Epoch 48/50\n",
      "897/897 [==============================] - 182s 203ms/step - loss: 0.2920 - accuracy: 0.8963\n",
      "Epoch 49/50\n",
      "897/897 [==============================] - 195s 218ms/step - loss: 0.2882 - accuracy: 0.8974\n",
      "Epoch 50/50\n",
      "897/897 [==============================] - 174s 194ms/step - loss: 0.2750 - accuracy: 0.9007\n"
     ]
    }
   ],
   "source": [
    "if mode == \"train\":\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=Adam(learning_rate=0.0001),metrics=['accuracy'])\n",
    "    model_info = model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=num_train // batch_size,\n",
    "            epochs=num_epoch,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=num_val // batch_size)\n",
    "    model.save('32_batch_ori_exported_model_git.h5')\n",
    "    \n",
    "# emotions will be displayed on your face from the webcam feed\n",
    "elif mode == \"display\":\n",
    "    model.load_weights('exported_model_git.h5')\n",
    "\n",
    "    # prevents openCL usage and unnecessary logging messages\n",
    "    cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "    # dictionary which assigns each label an emotion (alphabetical order)\n",
    "    emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "\n",
    "    # start the webcam feed\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while True:\n",
    "        # Find haar cascade to draw bounding box around face\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        facecasc = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = facecasc.detectMultiScale(gray,scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "            roi_gray = gray[y:y + h, x:x + w]\n",
    "            cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), -1), 0)\n",
    "            prediction = model.predict(cropped_img)\n",
    "            maxindex = int(np.argmax(prediction))\n",
    "            cv2.putText(frame, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('Video', cv2.resize(frame,(1600,960),interpolation = cv2.INTER_CUBIC))\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
