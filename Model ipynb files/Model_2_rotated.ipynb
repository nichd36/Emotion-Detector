{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import img_to_array, array_to_img\n",
    "from keras.utils import to_categorical\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.utils import resample\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set device placement to CPU\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "# Rest of your code for model training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the mode to either train or display and changing the batch size and number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"train\"\n",
    "batch_size = 32\n",
    "num_epoch = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the images for training into lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder = '/Users/nichdylan/Documents/Image Processing/images/train'\n",
    "\n",
    "image_lists = {}\n",
    "\n",
    "for subfolder in os.listdir(main_folder):\n",
    "    subfolder_path = os.path.join(main_folder, subfolder)\n",
    "    \n",
    "    if os.path.isdir(subfolder_path) and subfolder != \".DS_Store\":\n",
    "        image_list = []\n",
    "        \n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "                image_path = os.path.join(subfolder_path, filename)\n",
    "                image = cv2.imread(image_path)\n",
    "                \n",
    "                if image is not None:\n",
    "                    image_list.append(image)\n",
    "\n",
    "        image_lists[subfolder] = image_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling the disgust class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of \"disgust\" images before oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436\n"
     ]
    }
   ],
   "source": [
    "print(len(image_lists['disgust']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_lists['disgust'] = resample(image_lists['disgust'], n_samples=len(image_lists['fear']), random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of \"disgust\" images after oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436\n"
     ]
    }
   ],
   "source": [
    "print(len(image_lists['disgust']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the number of images for each of the 7 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List Name: happy, Type: <class 'list'>, Length: 7164\n",
      "List Name: sad, Type: <class 'list'>, Length: 4938\n",
      "List Name: fear, Type: <class 'list'>, Length: 4103\n",
      "List Name: surprise, Type: <class 'list'>, Length: 3205\n",
      "List Name: neutral, Type: <class 'list'>, Length: 4982\n",
      "List Name: angry, Type: <class 'list'>, Length: 3993\n",
      "List Name: disgust, Type: <class 'list'>, Length: 436\n"
     ]
    }
   ],
   "source": [
    "for list_name, list_data in image_lists.items():\n",
    "    data_type = type(list_data)\n",
    "    list_length = len(list_data)\n",
    "    print(f\"List Name: {list_name}, Type: {data_type}, Length: {list_length}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the image (grayscale, rescale, change label formatting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "\n",
    "for emotion, image_array_list in image_lists.items():\n",
    "    data.extend(image_array_list)\n",
    "    labels.extend([emotion] * len(image_array_list))\n",
    "\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "target_size = (48, 48)\n",
    "resized_data = [array_to_img(img, data_format=\"channels_last\").resize(target_size) for img in data]\n",
    "\n",
    "resized_data = [img_to_array(img, data_format=\"channels_last\") for img in resized_data]\n",
    "resized_data = np.array(resized_data)\n",
    "\n",
    "# A colored list would have a shape of (32488, 48, 48, 3)\n",
    "# Make a list for grayscaled_images, it would have the shape of (32488, 48, 48, 1)\n",
    "grayscale_images = []\n",
    "\n",
    "# Loop through the colored images and convert them to grayscaled\n",
    "for color_image in resized_data:\n",
    "    grayscale_image = cv2.cvtColor(color_image, cv2.COLOR_RGB2GRAY)\n",
    "    grayscale_image = np.expand_dims(grayscale_image, axis=-1)\n",
    "    grayscale_images.append(grayscale_image)\n",
    "\n",
    "# Convert the list of grayscale images to a NumPy array\n",
    "grayscale_images = np.array(grayscale_images)\n",
    "\n",
    "\n",
    "emotion_dict = {\n",
    "    \"angry\": 0,\n",
    "    \"disgust\": 1,\n",
    "    \"fear\": 2,\n",
    "    \"happy\": 3,\n",
    "    \"neutral\": 4,\n",
    "    \"sad\": 5,\n",
    "    \"surprise\": 6\n",
    "}\n",
    "\n",
    "# Map emotion names to numerical values\n",
    "numeric_labels = np.array([emotion_dict[label] for label in labels])\n",
    "numeric_labels = to_categorical(numeric_labels, num_classes=7)  # Assuming you have 7 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the training generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow(\n",
    "    x=grayscale_images,  # Image data\n",
    "    y=numeric_labels,  # Labels (emotions)\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the validation generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7066 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "val_dir = 'images/validation'\n",
    "\n",
    "num_train = 28709\n",
    "num_val = 7178\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=batch_size,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When mode = train, it will train the model. If model = display, it will open up the camera and recognize the emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "897/897 [==============================] - ETA: 0s - loss: 1.7893 - accuracy: 0.2629WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 224 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 224 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "897/897 [==============================] - 86s 95ms/step - loss: 1.7893 - accuracy: 0.2629 - val_loss: 1.6558 - val_accuracy: 0.3616\n",
      "Epoch 2/50\n",
      "897/897 [==============================] - 96s 107ms/step - loss: 1.6365 - accuracy: 0.3604\n",
      "Epoch 3/50\n",
      "897/897 [==============================] - 94s 105ms/step - loss: 1.5439 - accuracy: 0.4039\n",
      "Epoch 4/50\n",
      "897/897 [==============================] - 124s 138ms/step - loss: 1.4748 - accuracy: 0.4333\n",
      "Epoch 5/50\n",
      "897/897 [==============================] - 96s 107ms/step - loss: 1.4194 - accuracy: 0.4554\n",
      "Epoch 6/50\n",
      "897/897 [==============================] - 92s 102ms/step - loss: 1.3723 - accuracy: 0.4748\n",
      "Epoch 7/50\n",
      "897/897 [==============================] - 97s 108ms/step - loss: 1.3355 - accuracy: 0.4899\n",
      "Epoch 8/50\n",
      "897/897 [==============================] - 104s 116ms/step - loss: 1.2999 - accuracy: 0.5063\n",
      "Epoch 9/50\n",
      "897/897 [==============================] - 101s 112ms/step - loss: 1.2744 - accuracy: 0.5161\n",
      "Epoch 10/50\n",
      "897/897 [==============================] - 98s 109ms/step - loss: 1.2468 - accuracy: 0.5295\n",
      "Epoch 11/50\n",
      "897/897 [==============================] - 114s 128ms/step - loss: 1.2268 - accuracy: 0.5369\n",
      "Epoch 12/50\n",
      "897/897 [==============================] - 109s 122ms/step - loss: 1.2059 - accuracy: 0.5418\n",
      "Epoch 13/50\n",
      "897/897 [==============================] - 100s 112ms/step - loss: 1.1813 - accuracy: 0.5501\n",
      "Epoch 14/50\n",
      "897/897 [==============================] - 98s 109ms/step - loss: 1.1634 - accuracy: 0.5621\n",
      "Epoch 15/50\n",
      "897/897 [==============================] - 96s 107ms/step - loss: 1.1530 - accuracy: 0.5656\n",
      "Epoch 16/50\n",
      "897/897 [==============================] - 98s 110ms/step - loss: 1.1352 - accuracy: 0.5715\n",
      "Epoch 17/50\n",
      "897/897 [==============================] - 99s 111ms/step - loss: 1.1196 - accuracy: 0.5802\n",
      "Epoch 18/50\n",
      "897/897 [==============================] - 102s 114ms/step - loss: 1.1103 - accuracy: 0.5839\n",
      "Epoch 19/50\n",
      "897/897 [==============================] - 106s 119ms/step - loss: 1.1016 - accuracy: 0.5861\n",
      "Epoch 20/50\n",
      "897/897 [==============================] - 102s 114ms/step - loss: 1.0858 - accuracy: 0.5910\n",
      "Epoch 21/50\n",
      "897/897 [==============================] - 113s 126ms/step - loss: 1.0708 - accuracy: 0.5990\n",
      "Epoch 22/50\n",
      "897/897 [==============================] - 147s 163ms/step - loss: 1.0644 - accuracy: 0.6020\n",
      "Epoch 23/50\n",
      "897/897 [==============================] - 142s 158ms/step - loss: 1.0507 - accuracy: 0.6050\n",
      "Epoch 24/50\n",
      "897/897 [==============================] - 143s 160ms/step - loss: 1.0390 - accuracy: 0.6142\n",
      "Epoch 25/50\n",
      "897/897 [==============================] - 146s 162ms/step - loss: 1.0196 - accuracy: 0.6166\n",
      "Epoch 26/50\n",
      "897/897 [==============================] - 162s 180ms/step - loss: 1.0094 - accuracy: 0.6241\n",
      "Epoch 27/50\n",
      "897/897 [==============================] - 150s 167ms/step - loss: 0.9949 - accuracy: 0.6310\n",
      "Epoch 28/50\n",
      "897/897 [==============================] - 144s 160ms/step - loss: 0.9939 - accuracy: 0.6297\n",
      "Epoch 29/50\n",
      "897/897 [==============================] - 154s 172ms/step - loss: 0.9832 - accuracy: 0.6317\n",
      "Epoch 30/50\n",
      "897/897 [==============================] - 152s 170ms/step - loss: 0.9716 - accuracy: 0.6376\n",
      "Epoch 31/50\n",
      "897/897 [==============================] - 150s 167ms/step - loss: 0.9583 - accuracy: 0.6459\n",
      "Epoch 32/50\n",
      "897/897 [==============================] - 164s 183ms/step - loss: 0.9539 - accuracy: 0.6463\n",
      "Epoch 33/50\n",
      "897/897 [==============================] - 157s 175ms/step - loss: 0.9376 - accuracy: 0.6521\n",
      "Epoch 34/50\n",
      "897/897 [==============================] - 153s 171ms/step - loss: 0.9394 - accuracy: 0.6537\n",
      "Epoch 35/50\n",
      "897/897 [==============================] - 138s 154ms/step - loss: 0.9238 - accuracy: 0.6560\n",
      "Epoch 36/50\n",
      "897/897 [==============================] - 135s 151ms/step - loss: 0.9157 - accuracy: 0.6592\n",
      "Epoch 37/50\n",
      "897/897 [==============================] - 114s 127ms/step - loss: 0.9064 - accuracy: 0.6631\n",
      "Epoch 38/50\n",
      "897/897 [==============================] - 137s 153ms/step - loss: 0.8933 - accuracy: 0.6701\n",
      "Epoch 39/50\n",
      "897/897 [==============================] - 147s 164ms/step - loss: 0.8863 - accuracy: 0.6691\n",
      "Epoch 40/50\n",
      "897/897 [==============================] - 142s 158ms/step - loss: 0.8733 - accuracy: 0.6785\n",
      "Epoch 41/50\n",
      "897/897 [==============================] - 125s 139ms/step - loss: 0.8698 - accuracy: 0.6795\n",
      "Epoch 42/50\n",
      "897/897 [==============================] - 120s 134ms/step - loss: 0.8568 - accuracy: 0.6825\n",
      "Epoch 43/50\n",
      "897/897 [==============================] - 116s 130ms/step - loss: 0.8482 - accuracy: 0.6872\n",
      "Epoch 44/50\n",
      "897/897 [==============================] - 114s 127ms/step - loss: 0.8388 - accuracy: 0.6883\n",
      "Epoch 45/50\n",
      "897/897 [==============================] - 114s 127ms/step - loss: 0.8323 - accuracy: 0.6950\n",
      "Epoch 46/50\n",
      "897/897 [==============================] - 117s 131ms/step - loss: 0.8293 - accuracy: 0.6932\n",
      "Epoch 47/50\n",
      "897/897 [==============================] - 119s 133ms/step - loss: 0.8098 - accuracy: 0.7016\n",
      "Epoch 48/50\n",
      "897/897 [==============================] - 118s 132ms/step - loss: 0.8017 - accuracy: 0.7055\n",
      "Epoch 49/50\n",
      "897/897 [==============================] - 120s 134ms/step - loss: 0.7908 - accuracy: 0.7111\n",
      "Epoch 50/50\n",
      "897/897 [==============================] - 124s 138ms/step - loss: 0.7897 - accuracy: 0.7101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nichdylan/Documents/Image Processing/image/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "if mode == \"train\":\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=Adam(learning_rate=0.0001),metrics=['accuracy'])\n",
    "    model_info = model.fit(\n",
    "            train_generator,\n",
    "            steps_per_epoch=num_train // batch_size,\n",
    "            epochs=num_epoch,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=num_val // batch_size)\n",
    "    model.save('Model2_30ONLYrotate_num_train_changed_32_batch_exported_model_git.h5')\n",
    "    \n",
    "# emotions will be displayed on your face from the webcam feed\n",
    "elif mode == \"display\":\n",
    "    model.load_weights('ONLYrotate_num_train_changed_32_batch_exported_model_git.h5')\n",
    "\n",
    "    # prevents openCL usage and unnecessary logging messages\n",
    "    cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "    # dictionary which assigns each label an emotion (alphabetical order)\n",
    "    emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "\n",
    "    # start the webcam feed\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while True:\n",
    "        # Find haar cascade to draw bounding box around face\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        facecasc = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = facecasc.detectMultiScale(gray,scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "            roi_gray = gray[y:y + h, x:x + w]\n",
    "            cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), -1), 0)\n",
    "            prediction = model.predict(cropped_img)\n",
    "            maxindex = int(np.argmax(prediction))\n",
    "            cv2.putText(frame, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('Video', cv2.resize(frame,(1600,960),interpolation = cv2.INTER_CUBIC))\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
