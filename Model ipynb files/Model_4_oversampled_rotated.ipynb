{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import img_to_array, array_to_img\n",
    "from keras.utils import to_categorical\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.utils import resample\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set device placement to CPU\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "# Rest of your code for model training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the mode to either train or display and changing the batch size and number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"train\"\n",
    "batch_size = 32\n",
    "num_epoch = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the images for training into lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder = '/Users/nichdylan/Documents/Image Processing/images/train'\n",
    "\n",
    "image_lists = {}\n",
    "\n",
    "for subfolder in os.listdir(main_folder):\n",
    "    subfolder_path = os.path.join(main_folder, subfolder)\n",
    "    \n",
    "    if os.path.isdir(subfolder_path) and subfolder != \".DS_Store\":\n",
    "        image_list = []\n",
    "        \n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "                image_path = os.path.join(subfolder_path, filename)\n",
    "                image = cv2.imread(image_path)\n",
    "                \n",
    "                if image is not None:\n",
    "                    image_list.append(image)\n",
    "\n",
    "        image_lists[subfolder] = image_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling the disgust class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of \"disgust\" images before oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436\n"
     ]
    }
   ],
   "source": [
    "print(len(image_lists['disgust']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_lists['disgust'] = resample(image_lists['disgust'], n_samples=len(image_lists['fear']), random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of \"disgust\" images after oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4103\n"
     ]
    }
   ],
   "source": [
    "print(len(image_lists['disgust']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the number of images for each of the 7 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List Name: happy, Type: <class 'list'>, Length: 7164\n",
      "List Name: sad, Type: <class 'list'>, Length: 4938\n",
      "List Name: fear, Type: <class 'list'>, Length: 4103\n",
      "List Name: surprise, Type: <class 'list'>, Length: 3205\n",
      "List Name: neutral, Type: <class 'list'>, Length: 4982\n",
      "List Name: angry, Type: <class 'list'>, Length: 3993\n",
      "List Name: disgust, Type: <class 'list'>, Length: 4103\n"
     ]
    }
   ],
   "source": [
    "for list_name, list_data in image_lists.items():\n",
    "    data_type = type(list_data)\n",
    "    list_length = len(list_data)\n",
    "    print(f\"List Name: {list_name}, Type: {data_type}, Length: {list_length}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the image (grayscale, rescale, change label formatting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grayscale images shape: (32488, 48, 48, 1)\n",
      "\n",
      "\n",
      "DONEE\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n",
      "X shape: (32488, 48, 48, 1)\n",
      "y shape: (32488, 7)\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "labels = []\n",
    "\n",
    "for emotion, image_array_list in image_lists.items():\n",
    "    data.extend(image_array_list)\n",
    "    labels.extend([emotion] * len(image_array_list))\n",
    "\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "target_size = (48, 48)\n",
    "resized_data = [array_to_img(img, data_format=\"channels_last\").resize(target_size) for img in data]\n",
    "\n",
    "resized_data = [img_to_array(img, data_format=\"channels_last\") for img in resized_data]\n",
    "resized_data = np.array(resized_data)\n",
    "\n",
    "# A colored list would have a shape of (32488, 48, 48, 3)\n",
    "# Make a list for grayscaled_images, it would have the shape of (32488, 48, 48, 1)\n",
    "grayscale_images = []\n",
    "\n",
    "# Loop through the colored images and convert them to grayscaled\n",
    "for color_image in resized_data:\n",
    "    grayscale_image = cv2.cvtColor(color_image, cv2.COLOR_RGB2GRAY)\n",
    "    grayscale_image = np.expand_dims(grayscale_image, axis=-1)\n",
    "    grayscale_images.append(grayscale_image)\n",
    "\n",
    "# Convert the list of grayscale images to a NumPy array\n",
    "grayscale_images = np.array(grayscale_images)\n",
    "\n",
    "\n",
    "emotion_dict = {\n",
    "    \"angry\": 0,\n",
    "    \"disgust\": 1,\n",
    "    \"fear\": 2,\n",
    "    \"happy\": 3,\n",
    "    \"neutral\": 4,\n",
    "    \"sad\": 5,\n",
    "    \"surprise\": 6\n",
    "}\n",
    "\n",
    "# Map emotion names to numerical values\n",
    "numeric_labels = np.array([emotion_dict[label] for label in labels])\n",
    "numeric_labels = to_categorical(numeric_labels, num_classes=7)  # Assuming you have 7 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the training generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow(\n",
    "    x=grayscale_images,  # Image data\n",
    "    y=numeric_labels,  # Labels (emotions)\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the validation generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7066 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "val_dir = 'images/validation'\n",
    "\n",
    "num_train = 32533\n",
    "num_val = 7178\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=batch_size,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When mode = train, it will train the model. If model = display, it will open up the camera and recognize the emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1016/1016 [==============================] - ETA: 0s - loss: 1.8734 - accuracy: 0.2472WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 224 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 224 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1016/1016 [==============================] - 95s 94ms/step - loss: 1.8734 - accuracy: 0.2472 - val_loss: 1.7585 - val_accuracy: 0.3108\n",
      "Epoch 2/50\n",
      "1016/1016 [==============================] - 102s 100ms/step - loss: 1.7002 - accuracy: 0.3485\n",
      "Epoch 3/50\n",
      "1016/1016 [==============================] - 117s 115ms/step - loss: 1.5774 - accuracy: 0.4036\n",
      "Epoch 4/50\n",
      "1016/1016 [==============================] - 120s 118ms/step - loss: 1.4861 - accuracy: 0.4424\n",
      "Epoch 5/50\n",
      "1016/1016 [==============================] - 133s 131ms/step - loss: 1.4088 - accuracy: 0.4742\n",
      "Epoch 6/50\n",
      "1016/1016 [==============================] - 132s 130ms/step - loss: 1.3514 - accuracy: 0.4951\n",
      "Epoch 7/50\n",
      "1016/1016 [==============================] - 151s 149ms/step - loss: 1.3002 - accuracy: 0.5165\n",
      "Epoch 8/50\n",
      "1016/1016 [==============================] - 128s 126ms/step - loss: 1.2607 - accuracy: 0.5324\n",
      "Epoch 9/50\n",
      "1016/1016 [==============================] - 138s 135ms/step - loss: 1.2259 - accuracy: 0.5446\n",
      "Epoch 10/50\n",
      "1016/1016 [==============================] - 135s 133ms/step - loss: 1.1927 - accuracy: 0.5547\n",
      "Epoch 11/50\n",
      "1016/1016 [==============================] - 130s 128ms/step - loss: 1.1637 - accuracy: 0.5651\n",
      "Epoch 12/50\n",
      "1016/1016 [==============================] - 138s 136ms/step - loss: 1.1334 - accuracy: 0.5798\n",
      "Epoch 13/50\n",
      "1016/1016 [==============================] - 134s 132ms/step - loss: 1.1102 - accuracy: 0.5871\n",
      "Epoch 14/50\n",
      "1016/1016 [==============================] - 130s 128ms/step - loss: 1.0871 - accuracy: 0.5945\n",
      "Epoch 15/50\n",
      "1016/1016 [==============================] - 137s 135ms/step - loss: 1.0656 - accuracy: 0.6047\n",
      "Epoch 16/50\n",
      "1016/1016 [==============================] - 132s 130ms/step - loss: 1.0489 - accuracy: 0.6103\n",
      "Epoch 17/50\n",
      "1016/1016 [==============================] - 125s 123ms/step - loss: 1.0289 - accuracy: 0.6183\n",
      "Epoch 18/50\n",
      "1016/1016 [==============================] - 127s 125ms/step - loss: 1.0107 - accuracy: 0.6224\n",
      "Epoch 19/50\n",
      "1016/1016 [==============================] - 132s 130ms/step - loss: 0.9929 - accuracy: 0.6296\n",
      "Epoch 20/50\n",
      "1016/1016 [==============================] - 124s 122ms/step - loss: 0.9814 - accuracy: 0.6344\n",
      "Epoch 21/50\n",
      "1016/1016 [==============================] - 123s 121ms/step - loss: 0.9626 - accuracy: 0.6394\n",
      "Epoch 22/50\n",
      "1016/1016 [==============================] - 124s 122ms/step - loss: 0.9470 - accuracy: 0.6498\n",
      "Epoch 23/50\n",
      "1016/1016 [==============================] - 132s 130ms/step - loss: 0.9358 - accuracy: 0.6507\n",
      "Epoch 24/50\n",
      "1016/1016 [==============================] - 125s 123ms/step - loss: 0.9262 - accuracy: 0.6553\n",
      "Epoch 25/50\n",
      "1016/1016 [==============================] - 134s 132ms/step - loss: 0.9104 - accuracy: 0.6600\n",
      "Epoch 26/50\n",
      "1016/1016 [==============================] - 127s 125ms/step - loss: 0.9029 - accuracy: 0.6641\n",
      "Epoch 27/50\n",
      "1016/1016 [==============================] - 126s 125ms/step - loss: 0.8878 - accuracy: 0.6692\n",
      "Epoch 28/50\n",
      "1016/1016 [==============================] - 132s 130ms/step - loss: 0.8770 - accuracy: 0.6755\n",
      "Epoch 29/50\n",
      "1016/1016 [==============================] - 126s 124ms/step - loss: 0.8663 - accuracy: 0.6770\n",
      "Epoch 30/50\n",
      "1016/1016 [==============================] - 122s 120ms/step - loss: 0.8540 - accuracy: 0.6849\n",
      "Epoch 31/50\n",
      "1016/1016 [==============================] - 130s 128ms/step - loss: 0.8415 - accuracy: 0.6860\n",
      "Epoch 32/50\n",
      "1016/1016 [==============================] - 121s 119ms/step - loss: 0.8334 - accuracy: 0.6915\n",
      "Epoch 33/50\n",
      "1016/1016 [==============================] - 120s 118ms/step - loss: 0.8225 - accuracy: 0.6951\n",
      "Epoch 34/50\n",
      "1016/1016 [==============================] - 123s 121ms/step - loss: 0.8104 - accuracy: 0.7001\n",
      "Epoch 35/50\n",
      "1016/1016 [==============================] - 126s 124ms/step - loss: 0.8028 - accuracy: 0.7002\n",
      "Epoch 36/50\n",
      "1016/1016 [==============================] - 136s 134ms/step - loss: 0.7934 - accuracy: 0.7042\n",
      "Epoch 37/50\n",
      "1016/1016 [==============================] - 137s 135ms/step - loss: 0.7830 - accuracy: 0.7093\n",
      "Epoch 38/50\n",
      "1016/1016 [==============================] - 142s 140ms/step - loss: 0.7790 - accuracy: 0.7099\n",
      "Epoch 39/50\n",
      "1016/1016 [==============================] - 149s 146ms/step - loss: 0.7703 - accuracy: 0.7144\n",
      "Epoch 40/50\n",
      "1016/1016 [==============================] - 140s 138ms/step - loss: 0.7660 - accuracy: 0.7174\n",
      "Epoch 41/50\n",
      "1016/1016 [==============================] - 147s 145ms/step - loss: 0.7489 - accuracy: 0.7250\n",
      "Epoch 42/50\n",
      "1016/1016 [==============================] - 138s 136ms/step - loss: 0.7441 - accuracy: 0.7253\n",
      "Epoch 43/50\n",
      "1016/1016 [==============================] - 137s 135ms/step - loss: 0.7342 - accuracy: 0.7276\n",
      "Epoch 44/50\n",
      "1016/1016 [==============================] - 137s 135ms/step - loss: 0.7281 - accuracy: 0.7297\n",
      "Epoch 45/50\n",
      "1016/1016 [==============================] - 154s 152ms/step - loss: 0.7174 - accuracy: 0.7350\n",
      "Epoch 46/50\n",
      "1016/1016 [==============================] - 130s 128ms/step - loss: 0.7103 - accuracy: 0.7372\n",
      "Epoch 47/50\n",
      "1016/1016 [==============================] - 137s 135ms/step - loss: 0.7059 - accuracy: 0.7372\n",
      "Epoch 48/50\n",
      "1016/1016 [==============================] - 150s 147ms/step - loss: 0.7011 - accuracy: 0.7427\n",
      "Epoch 49/50\n",
      "1016/1016 [==============================] - 156s 153ms/step - loss: 0.6893 - accuracy: 0.7463\n",
      "Epoch 50/50\n",
      "1016/1016 [==============================] - 152s 149ms/step - loss: 0.6793 - accuracy: 0.7480\n"
     ]
    }
   ],
   "source": [
    "if mode == \"train\":\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=Adam(learning_rate=0.0001),metrics=['accuracy'])\n",
    "    model_info = model.fit(\n",
    "            train_generator,\n",
    "            steps_per_epoch=num_train // batch_size,\n",
    "            epochs=num_epoch,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=num_val // batch_size)\n",
    "    model.save('rotate_num_train_changed_32_batch_OS_exported_model_git.h5')\n",
    "    \n",
    "# emotions will be displayed on your face from the webcam feed\n",
    "elif mode == \"display\":\n",
    "    model.load_weights('exported_model_git.h5')\n",
    "\n",
    "    # prevents openCL usage and unnecessary logging messages\n",
    "    cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "    # dictionary which assigns each label an emotion (alphabetical order)\n",
    "    emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "\n",
    "    # start the webcam feed\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while True:\n",
    "        # Find haar cascade to draw bounding box around face\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        facecasc = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = facecasc.detectMultiScale(gray,scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "            roi_gray = gray[y:y + h, x:x + w]\n",
    "            cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), -1), 0)\n",
    "            prediction = model.predict(cropped_img)\n",
    "            maxindex = int(np.argmax(prediction))\n",
    "            cv2.putText(frame, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('Video', cv2.resize(frame,(1600,960),interpolation = cv2.INTER_CUBIC))\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
