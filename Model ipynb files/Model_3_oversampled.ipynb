{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import img_to_array, array_to_img\n",
    "from keras.preprocessing.image import img_to_array, array_to_img\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.utils import resample\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the mode to either train or display and changing the batch size and number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"train\"\n",
    "batch_size = 32\n",
    "num_epoch = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the images for training into lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder = '/Users/nichdylan/Documents/Image Processing/images/train'\n",
    "\n",
    "image_lists = {}\n",
    "\n",
    "for subfolder in os.listdir(main_folder):\n",
    "    subfolder_path = os.path.join(main_folder, subfolder)\n",
    "    \n",
    "    if os.path.isdir(subfolder_path) and subfolder != \".DS_Store\":\n",
    "        image_list = []\n",
    "        \n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "                image_path = os.path.join(subfolder_path, filename)\n",
    "                image = cv2.imread(image_path)\n",
    "                \n",
    "                if image is not None:\n",
    "                    image_list.append(image)\n",
    "\n",
    "        image_lists[subfolder] = image_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling the disgust class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of \"disgust\" images before oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436\n"
     ]
    }
   ],
   "source": [
    "print(len(image_lists['disgust']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_lists['disgust'] = resample(image_lists['disgust'], n_samples=len(image_lists['fear']), random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of \"disgust\" images after oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4103\n"
     ]
    }
   ],
   "source": [
    "print(len(image_lists['disgust']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the number of images for each of the 7 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List Name: happy, Type: <class 'list'>, Length: 7164\n",
      "List Name: sad, Type: <class 'list'>, Length: 4938\n",
      "List Name: fear, Type: <class 'list'>, Length: 4103\n",
      "List Name: surprise, Type: <class 'list'>, Length: 3205\n",
      "List Name: neutral, Type: <class 'list'>, Length: 4982\n",
      "List Name: angry, Type: <class 'list'>, Length: 3993\n",
      "List Name: disgust, Type: <class 'list'>, Length: 4103\n"
     ]
    }
   ],
   "source": [
    "for list_name, list_data in image_lists.items():\n",
    "    data_type = type(list_data)\n",
    "    list_length = len(list_data)\n",
    "    print(f\"List Name: {list_name}, Type: {data_type}, Length: {list_length}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the image (grayscale, rescale, change label formatting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "\n",
    "for emotion, image_array_list in image_lists.items():\n",
    "    data.extend(image_array_list)\n",
    "    labels.extend([emotion] * len(image_array_list))\n",
    "\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "target_size = (48, 48)\n",
    "resized_data = [array_to_img(img, data_format=\"channels_last\").resize(target_size) for img in data]\n",
    "\n",
    "resized_data = [img_to_array(img, data_format=\"channels_last\") for img in resized_data]\n",
    "resized_data = np.array(resized_data)\n",
    "\n",
    "# A colored list would have a shape of (32488, 48, 48, 3)\n",
    "# Make a list for grayscaled_images, it would have the shape of (32488, 48, 48, 1)\n",
    "grayscale_images = []\n",
    "\n",
    "# Loop through the colored images and convert them to grayscaled\n",
    "for color_image in resized_data:\n",
    "    grayscale_image = cv2.cvtColor(color_image, cv2.COLOR_RGB2GRAY)\n",
    "    # Reshape the grayscale image to (48, 48, 1)\n",
    "    grayscale_image = np.expand_dims(grayscale_image, axis=-1)\n",
    "    grayscale_images.append(grayscale_image)\n",
    "\n",
    "# Convert the list of grayscale images to a NumPy array\n",
    "grayscale_images = np.array(grayscale_images)\n",
    "\n",
    "\n",
    "emotion_dict = {\n",
    "    \"angry\": 0,\n",
    "    \"disgust\": 1,\n",
    "    \"fear\": 2,\n",
    "    \"happy\": 3,\n",
    "    \"neutral\": 4,\n",
    "    \"sad\": 5,\n",
    "    \"surprise\": 6\n",
    "}\n",
    "\n",
    "numeric_labels = np.array([emotion_dict[label] for label in labels])\n",
    "numeric_labels = to_categorical(numeric_labels, num_classes=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the training generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow(\n",
    "    x=grayscale_images,\n",
    "    y=numeric_labels,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the validation generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7066 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "val_dir = 'images/validation'\n",
    "\n",
    "num_train = 32533\n",
    "num_val = 7178\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=batch_size,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When mode = train, it will train the model. If model = display, it will open up the camera and recognize the emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1016/1016 [==============================] - ETA: 0s - loss: 1.8252 - accuracy: 0.2792WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 224 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 224 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1016/1016 [==============================] - 92s 90ms/step - loss: 1.8252 - accuracy: 0.2792 - val_loss: 1.6564 - val_accuracy: 0.3813\n",
      "Epoch 2/50\n",
      "1016/1016 [==============================] - 95s 94ms/step - loss: 1.5857 - accuracy: 0.4035\n",
      "Epoch 3/50\n",
      "1016/1016 [==============================] - 112s 110ms/step - loss: 1.4286 - accuracy: 0.4658\n",
      "Epoch 4/50\n",
      "1016/1016 [==============================] - 104s 102ms/step - loss: 1.3151 - accuracy: 0.5127\n",
      "Epoch 5/50\n",
      "1016/1016 [==============================] - 106s 105ms/step - loss: 1.2328 - accuracy: 0.5399\n",
      "Epoch 6/50\n",
      "1016/1016 [==============================] - 106s 104ms/step - loss: 1.1616 - accuracy: 0.5690\n",
      "Epoch 7/50\n",
      "1016/1016 [==============================] - 110s 108ms/step - loss: 1.1063 - accuracy: 0.5902\n",
      "Epoch 8/50\n",
      "1016/1016 [==============================] - 108s 106ms/step - loss: 1.0535 - accuracy: 0.6090\n",
      "Epoch 9/50\n",
      "1016/1016 [==============================] - 104s 102ms/step - loss: 1.0100 - accuracy: 0.6253\n",
      "Epoch 10/50\n",
      "1016/1016 [==============================] - 132s 130ms/step - loss: 0.9668 - accuracy: 0.6383\n",
      "Epoch 11/50\n",
      "1016/1016 [==============================] - 143s 141ms/step - loss: 0.9355 - accuracy: 0.6524\n",
      "Epoch 12/50\n",
      "1016/1016 [==============================] - 145s 143ms/step - loss: 0.8957 - accuracy: 0.6662\n",
      "Epoch 13/50\n",
      "1016/1016 [==============================] - 126s 124ms/step - loss: 0.8632 - accuracy: 0.6794\n",
      "Epoch 14/50\n",
      "1016/1016 [==============================] - 118s 117ms/step - loss: 0.8297 - accuracy: 0.6943\n",
      "Epoch 15/50\n",
      "1016/1016 [==============================] - 120s 119ms/step - loss: 0.8079 - accuracy: 0.7013\n",
      "Epoch 16/50\n",
      "1016/1016 [==============================] - 129s 127ms/step - loss: 0.7696 - accuracy: 0.7142\n",
      "Epoch 17/50\n",
      "1016/1016 [==============================] - 127s 125ms/step - loss: 0.7458 - accuracy: 0.7269\n",
      "Epoch 18/50\n",
      "1016/1016 [==============================] - 124s 122ms/step - loss: 0.7207 - accuracy: 0.7365\n",
      "Epoch 19/50\n",
      "1016/1016 [==============================] - 126s 124ms/step - loss: 0.6816 - accuracy: 0.7494\n",
      "Epoch 20/50\n",
      "1016/1016 [==============================] - 125s 123ms/step - loss: 0.6539 - accuracy: 0.7602\n",
      "Epoch 21/50\n",
      "1016/1016 [==============================] - 124s 122ms/step - loss: 0.6222 - accuracy: 0.7727\n",
      "Epoch 22/50\n",
      "1016/1016 [==============================] - 122s 120ms/step - loss: 0.6025 - accuracy: 0.7783\n",
      "Epoch 23/50\n",
      "1016/1016 [==============================] - 124s 122ms/step - loss: 0.5699 - accuracy: 0.7903\n",
      "Epoch 24/50\n",
      "1016/1016 [==============================] - 124s 122ms/step - loss: 0.5477 - accuracy: 0.7998\n",
      "Epoch 25/50\n",
      "1016/1016 [==============================] - 117s 116ms/step - loss: 0.5242 - accuracy: 0.8098\n",
      "Epoch 26/50\n",
      "1016/1016 [==============================] - 123s 121ms/step - loss: 0.4977 - accuracy: 0.8211\n",
      "Epoch 27/50\n",
      "1016/1016 [==============================] - 124s 122ms/step - loss: 0.4794 - accuracy: 0.8249\n",
      "Epoch 28/50\n",
      "1016/1016 [==============================] - 126s 124ms/step - loss: 0.4620 - accuracy: 0.8305\n",
      "Epoch 29/50\n",
      "1016/1016 [==============================] - 127s 125ms/step - loss: 0.4436 - accuracy: 0.8398\n",
      "Epoch 30/50\n",
      "1016/1016 [==============================] - 126s 124ms/step - loss: 0.4209 - accuracy: 0.8476\n",
      "Epoch 31/50\n",
      "1016/1016 [==============================] - 125s 123ms/step - loss: 0.4036 - accuracy: 0.8538\n",
      "Epoch 32/50\n",
      "1016/1016 [==============================] - 125s 123ms/step - loss: 0.3822 - accuracy: 0.8624\n",
      "Epoch 33/50\n",
      "1016/1016 [==============================] - 127s 125ms/step - loss: 0.3724 - accuracy: 0.8656\n",
      "Epoch 34/50\n",
      "1016/1016 [==============================] - 704s 694ms/step - loss: 0.3602 - accuracy: 0.8708\n",
      "Epoch 35/50\n",
      "1016/1016 [==============================] - 371s 366ms/step - loss: 0.3374 - accuracy: 0.8793\n",
      "Epoch 36/50\n",
      "1016/1016 [==============================] - 90s 89ms/step - loss: 0.3260 - accuracy: 0.8828\n",
      "Epoch 37/50\n",
      "1016/1016 [==============================] - 96s 95ms/step - loss: 0.3200 - accuracy: 0.8842\n",
      "Epoch 38/50\n",
      "1016/1016 [==============================] - 98s 97ms/step - loss: 0.3077 - accuracy: 0.8888\n",
      "Epoch 39/50\n",
      "1016/1016 [==============================] - 100s 99ms/step - loss: 0.2944 - accuracy: 0.8954\n",
      "Epoch 40/50\n",
      "1016/1016 [==============================] - 100s 99ms/step - loss: 0.2875 - accuracy: 0.8967\n",
      "Epoch 41/50\n",
      "1016/1016 [==============================] - 100s 98ms/step - loss: 0.2797 - accuracy: 0.9009\n",
      "Epoch 42/50\n",
      "1016/1016 [==============================] - 101s 100ms/step - loss: 0.2663 - accuracy: 0.9042\n",
      "Epoch 43/50\n",
      "1016/1016 [==============================] - 103s 101ms/step - loss: 0.2589 - accuracy: 0.9074\n",
      "Epoch 44/50\n",
      "1016/1016 [==============================] - 124s 122ms/step - loss: 0.2564 - accuracy: 0.9094\n",
      "Epoch 45/50\n",
      "1016/1016 [==============================] - 849s 836ms/step - loss: 0.2424 - accuracy: 0.9134\n",
      "Epoch 46/50\n",
      "1016/1016 [==============================] - 235s 231ms/step - loss: 0.2390 - accuracy: 0.9142\n",
      "Epoch 47/50\n",
      "1016/1016 [==============================] - 91s 89ms/step - loss: 0.2294 - accuracy: 0.9165\n",
      "Epoch 48/50\n",
      "1016/1016 [==============================] - 102s 101ms/step - loss: 0.2230 - accuracy: 0.9211\n",
      "Epoch 49/50\n",
      "1016/1016 [==============================] - 104s 102ms/step - loss: 0.2279 - accuracy: 0.9182\n",
      "Epoch 50/50\n",
      "1016/1016 [==============================] - 105s 103ms/step - loss: 0.2222 - accuracy: 0.9207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nichdylan/Documents/Image Processing/image/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "if mode == \"train\":\n",
    "    from keras.utils import to_categorical\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=Adam(learning_rate=0.0001),metrics=['accuracy'])\n",
    "    model_info = model.fit(\n",
    "            train_generator,\n",
    "            steps_per_epoch=num_train // batch_size,\n",
    "            epochs=num_epoch,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=num_val // batch_size)\n",
    "    model.save_weights('model.h5')\n",
    "    model.save('num_train_changed_32_batch_OS_exported_model_git.h5')\n",
    "    \n",
    "# emotions will be displayed on your face from the webcam feed\n",
    "elif mode == \"display\":\n",
    "    model.load_weights('num_train_changed_32_batch_OS_exported_model_git.h5')\n",
    "\n",
    "    # prevents openCL usage and unnecessary logging messages\n",
    "    cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "    # dictionary which assigns each label an emotion (alphabetical order)\n",
    "    emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "\n",
    "    # start the webcam feed\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while True:\n",
    "        # Find haar cascade to draw bounding box around face\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        facecasc = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = facecasc.detectMultiScale(gray,scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "            roi_gray = gray[y:y + h, x:x + w]\n",
    "            cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), -1), 0)\n",
    "            prediction = model.predict(cropped_img)\n",
    "            maxindex = int(np.argmax(prediction))\n",
    "            cv2.putText(frame, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('Video', cv2.resize(frame,(1600,960),interpolation = cv2.INTER_CUBIC))\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
